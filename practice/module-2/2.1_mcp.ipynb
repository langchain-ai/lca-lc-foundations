{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cdc3544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744bbdbb",
   "metadata": {},
   "source": [
    "## Local MCP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87864b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb836dc",
   "metadata": {},
   "source": [
    "Model Context Protocol:\n",
    "\n",
    "An open protocol that standardizes how your LLM applications connect to and work with your tools and data sources.\n",
    "MCP Host hosts and MCP Client that communicates with the MCP Server.\n",
    "\n",
    "MCP Host is like an Agent.\n",
    "MCP Server can expose tools, resources, and prompts to the client.\n",
    "The benefit of MCP server is that all the tools, resources, and prompts can be used in my project/use case without the need of defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2defd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82517ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b31ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd50535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86cc9a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='021330a0-a946-44dc-a9a3-41faddff8306'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 271, 'total_tokens': 363, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpPseI4r826PeXzoCB6Oe9lbepfJZ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b43df-0482-7ee2-9e46-5af5f6d44150-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_jTOM94SFKMSka9kIyjzvL21q', 'type': 'tool_call'}], usage_metadata={'input_tokens': 271, 'output_tokens': 92, 'total_tokens': 363, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.9999956,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters\",\\n      \"title\": \"LangChain MCP Integration: Complete Guide to ...\",\\n      \"content\": \"from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_command=[\\\\\"python\\\\\", \\\\\"database_mcp_server.py\\\\\"], transport_type=\\\\\"stdio\\\\\", environment={ \\\\\"DATABASE_URL\\\\\": \\\\\"postgresql://user:pass@localhost:5432/mydb\\\\\", \\\\\"MAX_CONNECTIONS\\\\\": \\\\\"10\\\\\" } ) await db_adapter.connect() db_tools = await db_adapter.get_tools() # Create an agent with database capabilities llm = ChatOpenAI(model=\\\\\"gpt-4\\\\\") agent = create_react_agent(llm, db_tools) # Execute SQL queries through MCP response = await agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Find all customers who made purchases over $500 in the last month\\\\\" }). import os # REST API MCP server integration api_adapter = MCPAdapter( url=\\\\\"http://localhost:3000/mcp\\\\\", transport_type=\\\\\"sse\\\\\", headers={ \\\\\"Authorization\\\\\": f\\\\\"Bearer {os.getenv(\\'API_TOKEN\\')}\\\\\", \\\\\"User-Agent\\\\\": \\\\\"LangChain-MCP-Client/1.0\\\\\" } ) api_tools = await api_adapter.get_tools() crm_agent = create_react_agent(llm, api_tools) # Use the agent to interact with the CRM API customer_data = await crm_agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Create a new lead for John Smith with email [email\\xa0protected]\\\\\" }). Instead of writing adapter code or managing MCP servers, Latenode users can connect AI agents to more than 350 external services using pre-built connectors and drag-and-drop workflows.\",\\n      \"score\": 0.9999621,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/langchain-mcp-adapters-0-2-0\",\\n      \"title\": \"LangChain MCP Adapters 0.2.0\",\\n      \"content\": \"LangChain - Changelog | LangChain MCP Adapters 0.2.0. Sign up for our newsletter to stay up to date. LangChain MCP Adapters 0.2.0. Image 1**LangChain MCP Adapters 0.2.0 is live.**This release brings quality-of-life upgrades for anyone building with MCP tools in LangChain:. Image 2**Multimodal tool support**. Use tools that accept and produce images, text, and other modalities‚Äîpowered by LangChain‚Äôs standard content blocks. Image 3**Elicitation support via callbacks**. Easily implement prompt-driven clarifications and multi-turn tool interactions. Image 4**Structured tool output as artifacts**. Tool results now store structured content as artifacts, making downstream processing and inspection much cleaner. Image 5**Tool name prefixes for multi-server setups**. Eliminate naming collisions and run multiple MCP servers seamlessly. We\\'ve also released new docs to help you get started fast: https://docs.langchain.com/oss/python/langchain/mcp. Technical release notes: https://github.com/langchain-ai/langchain-mcp-adapters/releases/tag/langchain-mcp-adapters%3D%3D0.2.0. ##### Subscribe to updates. Subscribe By clicking subscribe, you accept our privacy policy and terms and conditions. reCAPTCHA privacy and terms apply.\",\\n      \"score\": 0.9999535,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain üîå MCP\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\\\\\"openai:gpt-4.1\\\\\" client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"./examples/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \\\\\"messages\\\\\" return \\\\\"messages\\\\\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \\\\\"call_model\\\\\" builder add_conditional_edges \\\\\"call_model\\\\\" tools_condition builder add_edge \\\\\"tools\\\\\" \\\\\"call_model\\\\\" graph = builder compile math_response = await graph ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await graph ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\".\",\\n      \"score\": 0.9999461,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library. `langchain-mcp-adapters` enables agents to use tools defined across one or more MCP servers. To test your agent with MCP tool servers, use the following examples:. If you need to control the [lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`. Use `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:. MCP tools can return [structured content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#structured-content) alongside the human-readable text response. MCP tools can return [multimodal content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result) (images, text, etc.) in their responses. When MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. [Elicitation](https://modelcontextprotocol.io/specification/2025-11-25/client/elicitation#elicitation) allows MCP servers to request additional input from users during tool execution.\",\\n      \"score\": 0.99993896,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.0,\\n  \"request_id\": \"19f8e566-8ebb-4cf1-b1e8-d66f6efcff33\"\\n}', 'id': 'lc_9d67e61f-2759-4adb-960d-c37afb6d7ede'}], name='search_web', id='48ddf0dc-990b-415e-be59-9e0834032daa', tool_call_id='call_jTOM94SFKMSka9kIyjzvL21q', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.9999956, 'raw_content': None}, {'url': 'https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters', 'title': 'LangChain MCP Integration: Complete Guide to ...', 'content': 'from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_command=[\"python\", \"database_mcp_server.py\"], transport_type=\"stdio\", environment={ \"DATABASE_URL\": \"postgresql://user:pass@localhost:5432/mydb\", \"MAX_CONNECTIONS\": \"10\" } ) await db_adapter.connect() db_tools = await db_adapter.get_tools() # Create an agent with database capabilities llm = ChatOpenAI(model=\"gpt-4\") agent = create_react_agent(llm, db_tools) # Execute SQL queries through MCP response = await agent.ainvoke({ \"input\": \"Find all customers who made purchases over $500 in the last month\" }). import os # REST API MCP server integration api_adapter = MCPAdapter( url=\"http://localhost:3000/mcp\", transport_type=\"sse\", headers={ \"Authorization\": f\"Bearer {os.getenv(\\'API_TOKEN\\')}\", \"User-Agent\": \"LangChain-MCP-Client/1.0\" } ) api_tools = await api_adapter.get_tools() crm_agent = create_react_agent(llm, api_tools) # Use the agent to interact with the CRM API customer_data = await crm_agent.ainvoke({ \"input\": \"Create a new lead for John Smith with email [email\\xa0protected]\" }). Instead of writing adapter code or managing MCP servers, Latenode users can connect AI agents to more than 350 external services using pre-built connectors and drag-and-drop workflows.', 'score': 0.9999621, 'raw_content': None}, {'url': 'https://changelog.langchain.com/announcements/langchain-mcp-adapters-0-2-0', 'title': 'LangChain MCP Adapters 0.2.0', 'content': \"LangChain - Changelog | LangChain MCP Adapters 0.2.0. Sign up for our newsletter to stay up to date. LangChain MCP Adapters 0.2.0. Image 1**LangChain MCP Adapters 0.2.0 is live.**This release brings quality-of-life upgrades for anyone building with MCP tools in LangChain:. Image 2**Multimodal tool support**. Use tools that accept and produce images, text, and other modalities‚Äîpowered by LangChain‚Äôs standard content blocks. Image 3**Elicitation support via callbacks**. Easily implement prompt-driven clarifications and multi-turn tool interactions. Image 4**Structured tool output as artifacts**. Tool results now store structured content as artifacts, making downstream processing and inspection much cleaner. Image 5**Tool name prefixes for multi-server setups**. Eliminate naming collisions and run multiple MCP servers seamlessly. We've also released new docs to help you get started fast: https://docs.langchain.com/oss/python/langchain/mcp. Technical release notes: https://github.com/langchain-ai/langchain-mcp-adapters/releases/tag/langchain-mcp-adapters%3D%3D0.2.0. ##### Subscribe to updates. Subscribe By clicking subscribe, you accept our privacy policy and terms and conditions. reCAPTCHA privacy and terms apply.\", 'score': 0.9999535, 'raw_content': None}, {'url': 'https://github.com/langchain-ai/langchain-mcp-adapters', 'title': 'langchain-ai/langchain-mcp-adapters: LangChain üîå MCP', 'content': 'from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"/path/to/math_server.py\" \"transport\" \"stdio\" \"weather\" # Make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools agent = create_agent\"openai:gpt-4.1\" tools math_response = await agent ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await agent ainvoke \"messages\" \"what is the weather in nyc?\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\"openai:gpt-4.1\" client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"./examples/math_server.py\" \"transport\" \"stdio\" \"weather\" # make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \"messages\" return \"messages\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \"call_model\" builder add_conditional_edges \"call_model\" tools_condition builder add_edge \"tools\" \"call_model\" graph = builder compile math_response = await graph ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await graph ainvoke \"messages\" \"what is the weather in nyc?\".', 'score': 0.9999461, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/python/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': '[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library. `langchain-mcp-adapters` enables agents to use tools defined across one or more MCP servers. To test your agent with MCP tool servers, use the following examples:. If you need to control the [lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`. Use `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:. MCP tools can return [structured content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#structured-content) alongside the human-readable text response. MCP tools can return [multimodal content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result) (images, text, etc.) in their responses. When MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. [Elicitation](https://modelcontextprotocol.io/specification/2025-11-25/client/elicitation#elicitation) allows MCP servers to request additional input from users during tool execution.', 'score': 0.99993896, 'raw_content': None}], 'response_time': 0.0, 'request_id': '19f8e566-8ebb-4cf1-b1e8-d66f6efcff33'}}}),\n",
      "              AIMessage(content='Here‚Äôs a concise overview of the LangChain MCP Adapters library and what it‚Äôs for.\\n\\nWhat it is\\n- A package that makes it easy to use Anthropic‚Äôs Model Context Protocol (MCP) tools with LangChain and LangGraph.\\n- It converts MCP tools into LangChain- and LangGraph-compatible tools, enabling agents to call tools from MCP servers.\\n\\nWhy you‚Äôd use it\\n- You can connect LangChain/LangGraph agents to a large ecosystem of MCP tool servers (potentially hundreds of tools) without writing custom adapters for each server.\\n- It supports multi-server setups, so an agent can pull tools from multiple MCP servers and use them together.\\n- It‚Äôs designed to work well with modern MCP features like structured tool outputs and multimodal results, and it provides features for elicitation and interaction with tool results.\\n\\nKey features (as of MCP Adapters 0.2.0)\\n- Multimodal tool support: Tools can accept/produce images, text, and other modalities.\\n- Elicitation support via callbacks: Facilitates prompt-driven clarifications and multi-turn tool interactions.\\n- Structured tool output as artifacts: Tool results can be stored in a structured, inspectable form.\\n- Tool name prefixes for multi-server setups: Helps avoid naming collisions when you‚Äôre aggregating tools from several MCP servers.\\n- Documentation: New docs to help you get started fast.\\n- These features were highlighted in the LangChain changelog release notes for MCP Adapters 0.2.0.\\n\\nHow it works at a high level\\n- You connect to one or more MCP servers using an MCPAdapter (or a client like MultiServerMCPClient).\\n- You call get_tools() to retrieve the MCP-defined tools from those servers.\\n- You pass the retrieved tools to a LangChain (or LangGraph) agent (via create_agent or equivalent) so the agent can invoke them through MCP.\\n- This setup enables agents to use tools distributed across MCP servers and to handle structured or multimodal content returned by those tools.\\n- LangGraph usage: You can incorporate MCP tools into LangGraph workflows by creating ToolNodes and building graphs that route prompts to the appropriate MCP tools.\\n\\nWhere to learn more (recommended)\\n- LangChain changelog: MCP Adapters announcements and features (high-level overview and release details).\\n  - Example: ‚ÄúMCP Adapters for LangChain and LangGraph‚Äù changelog entry.\\n- Documentation (LangChain OSS docs):\\n  - Model Context Protocol (MCP) page explains how MCP works and how to use it with LangChain, including client.session(), client.get_tools(), and handling structured/multimodal outputs.\\n- GitHub repo:\\n  - langchain-ai/langchain-mcp-adapters provides the library code, usage examples, and install instructions.\\n- Additional guides and examples:\\n  - Blog/tutorial-style guides showing end-to-end MCP integration with LangChain (including code snippets and multi-server setups).\\n\\nA very high-level quick-start flow\\n- Install: pip install langchain-mcp-adapters\\n- Start or point at MCP servers and ensure they expose MCP tools.\\n- Create a client and fetch tools:\\n  - client = MultiServerMCPClient(...)\\n  - tools = await client.get_tools()\\n- Build a LangChain agent with those tools:\\n  - llm = ‚Ä¶ (e.g., a ChatOpenAI model)\\n  - agent = create_agent(llm, tools)\\n- Use the agent to invoke tools via MCP:\\n  - response = await agent.ainvoke({\"input\": \"Your query here\"})\\n\\nIf you‚Äôd like, I can pull up a concrete code example (from the docs or the repo) and tailor a minimal, working snippet for your environment (single MCP server vs. multi-server setup). Do you want a quick-start example for:\\n- a single MCP server,\\n- or a multi-server setup with LangGraph integration?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2393, 'prompt_tokens': 2326, 'total_tokens': 4719, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1600, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpPsh1551YlB3faiRO3rHrglK56d3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b43df-125e-76d2-a95e-d113a2b22d73-0', usage_metadata={'input_tokens': 2326, 'output_tokens': 2393, 'total_tokens': 4719, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1600}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
